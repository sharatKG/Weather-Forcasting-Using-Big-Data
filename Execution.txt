-------------------------------------MAP REDUCE-----------------------------
start-all.sh
sudo mkdir BA_RG
hdfs dfsadmin -safemode leave
sudo chmod -R 777 RG_BA
cd RG_BA


For mapping process :

SalesMapper.java
package SalesCountry;
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.*;
public class SalesMapper extends MapReduceBase implements Mapper <LongWritable, Text, Text,
IntWritable> {
 private final static IntWritable one = new IntWritable(1);
 public void map(LongWritable key, Text value, OutputCollector <Text, IntWritable> output,
Reporter reporter) throws IOException {
 String valueString = value.toString();
 String[] SingleCountryData = valueString.split(",");
 output.collect(new Text(SingleCountryData[7]), one);
 }
}



SalesCountryReducer.java
package SalesCountry;
import java.io.IOException;
import java.util.*;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.*;
public class SalesCountryReducer extends MapReduceBase implements Reducer<Text, IntWritable,
Text, IntWritable> {
 public void reduce(Text t_key, Iterator<IntWritable> values,
OutputCollector<Text,IntWritable> output, Reporter reporter) throws IOException {
 Text key = t_key;
 int frequencyForCountry = 0;
 while (values.hasNext()) {
 // replace type of value with the actual type of our value
 IntWritable value = (IntWritable) values.next();
 frequencyForCountry += value.get();

 }
 output.collect(key, new IntWritable(frequencyForCountry));
 }
}




SalesCountryDriver.java
package SalesCountry;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
public class SalesCountryDriver {
 public static void main(String[] args) {
 JobClient my_client = new JobClient();
 // Create a configuration object for the job
 JobConf job_conf = new JobConf(SalesCountryDriver.class);
 // Set a name of the Job
 job_conf.setJobName("SalePerCountry");
// Specify data type of output key and value
 job_conf.setOutputKeyClass(Text.class);
 job_conf.setOutputValueClass(IntWritable.class);
 // Specify names of Mapper and Reducer Class
 job_conf.setMapperClass(SalesCountry.SalesMapper.class);
 job_conf.setReducerClass(SalesCountry.SalesCountryReducer.class);
 // Specify formats of the data type of Input and output
 job_conf.setInputFormat(TextInputFormat.class);
 job_conf.setOutputFormat(TextOutputFormat.class);
 // Set input and output directories using command line arguments,
 //arg[0] = name of input directory on HDFS, and arg[1] = name of output directory to be
//created to store the output file.
 FileInputFormat.setInputPaths(job_conf, new Path(args[0]));
 FileOutputFormat.setOutputPath(job_conf, new Path(args[1]));
 my_client.setConf(job_conf);
 try {
 // Run the job
 JobClient.runJob(job_conf);
 } catch (Exception e) {
 e.printStackTrace();
 }
 }
}


ls -al
sudo chmod +r *.*


export CLASSPATH="$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-common2.8.1.jar:$HADOOP_HOME/share/hadoop/common/hadoop-common2.8.1.jar:~/MapReduceTutorial/SalesCountry/*:$HADOOP_HOME/lib/*"
javac -d . SalesMapper.java SalesCountryReducer.java SalesCountryDriver.java

sudo gedit Manifest.txt
Main-Class: SalesCountry.SalesCountryDriver

jar cfm ProductSalePerCountry.jar Manifest.txt SalesCountry/*.class
ls

stop-dfs.sh
stop-yarn.sh

$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/start-yarn.sh

hadoop fs -mkdir /inputMapReduce
hadoop fs -copyFromLocal Sales.csv /inputMapReduce

Step 7) Copy the File SalesJan2009.csv into ~/inputMapReduce
Now Use below command to copy ~/inputMapReduce to HDFS.
We can safely ignore this warning.
Verify whether a file is actually copied or not.

$HADOOP_HOME/bin/hdfs dfs -ls /inputMapReduce


$HADOOP_HOME/bin/hadoop jar ProductSalePerCountry.jar /inputMapReduce /mapreduce_output_sales

This will create an output directory named mapreduce_output_sales on HDFS. Contents of this
directory will be a file containing product sales per country

$HADOOP_HOME/bin/hdfs dfs -cat /mapreduce_output_sales/part-00000

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

----------------------------------------HIVE----------------------------------------------
hadoop fs -mkdir /2ch
hadoop fs -mkdir /2ch/lab2
hadoop fs -mkdir /2ch/lab2/SeasonResult
hadoop fs -mkdir /2ch/lab2/Teams

copy csv files to resp folders

create external table if not exists mm_team(
tname string,tid int )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ","
LINES TERMINATED BY "\n"
STORED AS TEXTFILE
LOCATION ""
TBLPROPERTIES ("skip.header.line.count"="1")


selecting only 10 cols
hive> select * from mm_seasons limit 10;

maximum of wins 
hive> select count(wteam_id) from mm_seasons;

no of wins
hive> select count(*) from mm_seasons where wloc="1";

max score
hive> select max(lscore) from mm_seasons;

avg score
hive> select avg(season) from mm_seasons;


sum of distinct values
hive> select sum(distinct lscore) from mm_seasons;

description of table
hive>  desc mm_seasons;

id in descending order whose score is less than 50
hive> select id,lteam_id from mm_seasons where wloc<50 order by id desc,lteam_id;

count of id whose score is greater than 50
select id,count(lteam_id) from mm_seasons where score>50 group by id,lteam_id;

hive> select id,lteam_id from mm_seasons where wloc<50 order by id,lteam_id desc;


-------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------PIG----------------------------------------------------------------

PigB

step 1: Go to hadoop home and put the txt file in it
---> cd /$Hadoop_Home/bin
---> hadoop fs -put /home/rahul/Downloads/Pdata.txt hdfs://localhost:9000/pig_data

step 2: to see the copy file [ go to hadoop_home] 
--->  cd $HADOOP_HOME/bin
--->  hdfs dfs -cat hdfs://localhost:9000/pig_data
---> cd ..
---> hadoop fs -ls /

step 3: start the server
---> mr-jobhistory-daemon.sh start historyserver

step 4: open new terminal

step 5: start the pig shell
---> pig
>>grunt will start 

step 6: load the data into variable
---> prg1 = LOAD 'hdfs://localhost:9000/pig_data' USING PigStorage(',') AS (emp_number:int , emp_name:chararray , job_title:chararray , dept_name:chararray, experience:int, salary:int);

---> describe prg1;

step 7: operation
---> e2 = order prg1 by salary desc;
---> dump e2;

---> e3 = order prg1 by emp_number desc;
---> dump e3;

---> e4 = order prg1 by salary asc;
---> dump e4;









